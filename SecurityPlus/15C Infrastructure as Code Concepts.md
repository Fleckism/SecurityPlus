---
tags: [firstTag, secondTag]
---
# EXAM OBJECTIVES COVERED

2.2 Summarize virtualization and cloud computing concepts

Coupled with the use of virtualization and the cloud is the idea of continuous delivery models for automation and service integration. These technologies can be used together to deliver an infrastructure as code model of provisioning networks and hosts to support application services.
# SERVICES INTEGRATION AND MICROSERVICES

In the early days of computer networks, architecture was focused on the provision of [[server machines and intermediate network systems (switches and routers)]]. Architectural choices centered around where to place a "box" to run monolithic network applications such as routing, security, address allocation, name resolution, file sharing, email, and so on. With virtualization, the provision of these applications is much less dependent on where you put the box and the OS that the box runs. Virtualization helps to make the design architecture fit to the business requirement rather than accommodate the business workflow to the platform requirement.

### Service-Oriented Architecture (SOA)

Service-oriented architecture ([[SOA]]) conceives of [[atomic services]] closely mapped to business workflows. **Each service takes defined inputs and produces defined outputs.** The service may itself be composed of sub-services. **The key features of a service function are that it is self-contained, does not rely on the state of other services, and exposes clear input/output (I/O) interfaces.** Because each service has a simple interface, interoperability is made much easier than with a complex monolithic application. The implementation of a service does not constrain compatibility choices for client services, which can use a different platform or development language. This independence of the service and the client requesting the service is referred to as loose coupling. 

### Microservices

Microservice-based development shares many similarities with Agile software project management and the processes of continuous delivery and deployment. It also shares roots with the **Unix philosophy that each program or tool should do one thing well.** The main difference between SOA and microservices is that SOA allows a service to be built from other services. By contrast, each microservice should be capable of being developed, tested, and deployed independently. **The microservices are said to be highly decoupled rather than just loosely decoupled.** 

### Services Integration and Orchestration

Services integration refers to ways of making these decoupled service or microservice components work together to perform a workflow. Where [[SOA]] used the concept of an enterprise service bus, microservices integration and cloud services/virtualization/automation integration generally is very often implemented using orchestration tools. Where [[automation]] focuses on making a single, discrete task easily repeatable, [[orchestration]] **performs a sequence of automated tasks**. For example, you might orchestrate adding a new VM to a load-balanced cluster. This end-to-end process might include provisioning the VM, configuring it, adding the new VM to the load-balanced cluster, and reconfiguring the load-balancing weight distribution given the new cluster configuration. **In doing this, the orchestrated steps would have to run numerous [[automated scripts or API service calls]].**

For orchestration to work properly, automated steps must occur in the right sequence, taking dependencies into account; it must provide the right security credentials at every step along the way; and it must have the rights and permissions to perform the defined tasks. Orchestration can automate processes that are complex, requiring dozens or hundreds of manual steps.

Cloud orchestration platforms connect to and provide administration, management, and orchestration for many popular cloud platforms and services. One of the advantages of using a third-party orchestration platform is protection from vendor lock in. If you wish to migrate from one cloud provider to another, or wish to move to a multi-cloud environment, automated workflows can often be adapted for use on new platforms. Industry leaders in this space include Chef ([chef.io](https://www.chef.io/)), Puppet ([puppet.com](https://puppet.com/)), Ansible ([ansible.com](https://www.ansible.com/)), and Kubernetes ([kubernetes.io](https://kubernetes.io/)).
# APPLICATION PROGRAMMING INTERFACES

Whether based on SOA or microservices, service integration, automation, and orchestration all depend on application programming interfaces ([[API]]s). The service API is the means by which external entities interact with the service, calling it with expected parameters and receiving the expected output. There are two predominant "styles" for creating web application APIs:

-   **Simple Object Access Protocol** (SOAP)—uses XML format messaging and has a number of extensions in the form of Web Services (WS) standards that support common features, such as authentication, transport security, and asynchronous messaging. SOAP also has a built-in error handling.
-   **Representational State Transfer** (REST)—where SOAP is a tightly specified protocol, REST is a looser architectural framework, also referred to as RESTful APIs. Where a SOAP request must be sent as a correctly formatted XML document, a REST request can be submitted as an HTTP operation/verb (GET or POST for example). Each resource or endpoint in the API, expressed as a noun, should be accessed via a single URL. 
# SERVERLESS ARCHITECTURE

Serverless is a modern design pattern for service delivery. It is strongly associated with modern web applications—most notably Netflix ([aws.amazon.com/solutions/case-studies/netflix-and-aws-lambda](https://aws.amazon.com/solutions/case-studies/netflix-and-aws-lambda/))—but providers are appearing with products to completely replace the concept of the corporate LAN. With serverless, all the architecture is hosted within a cloud, but unlike "traditional" virtual private cloud (VPC) offerings, services such as authentication, web applications, and communications aren't developed and managed as applications running on VM instances located within the cloud. Instead, the **applications are developed as functions and microservices**, each interacting with other functions to facilitate client requests. When the client requires some operation to be processed, the cloud spins up a container to run the code, performs the processing, and then destroys the container. Billing is based on execution time, rather than hourly charges. This type of service provision is also called function as a service ([[FaaS]]). FaaS products include AWS Lambda ([aws.amazon.com/lambda](https://aws.amazon.com/lambda/)), Google Cloud Functions ([cloud.google.com/functions](https://cloud.google.com/functions)), and Microsoft Azure Functions ([azure.microsoft.com/services/functions](https://azure.microsoft.com/en-us/services/functions/)).

The serverless paradigm eliminates the need to manage physical or virtual server instances, so there is no management effort for software and patches, administration privileges, or file system security monitoring. There is no requirement to provision multiple servers for redundancy or load balancing. As all of the processing is taking place within the cloud, there is little emphasis on the provision of a corporate network. This underlying architecture is managed by the service provider. The principal network security job is to ensure that the clients accessing the services have not been compromised in a way that allows a malicious actor to impersonate a legitimate user. This is a particularly important consideration for the developer accounts and devices used to update the application code underpinning the services. **These workstations must be fully locked down, running no other applications or web code than those necessary for development.**

Serverless does have considerable risks. As a new paradigm, use cases and best practices are not mature, especially as regards security. There is also a critical and unavoidable dependency on the service provider, with limited options for disaster recovery should that service provision fail.

**Serverless architecture depends heavily on the concept of event-driven orchestration to facilitate operations.** For example, [[when a client connects to an application, multiple services will be called to authenticate the user and device, identify the device location and address properties, create a session, load authorizations for the action, use application logic to process the action, read or commit information from a database, and write a log of the transaction. ]]This design logic is different from applications written to run in a "monolithic" server-based environment. This means that adapting existing corporate software will require substantial development effort.
# INFRASTRUCTURE AS CODE

The use of cloud technologies encourages the use of scripted approaches to provisioning, rather than manually making configuration changes, or installing patches. An approach to infrastructure management where automation and orchestration fully replace manual configuration is referred to as infrastructure as code ([[IaC]]).

One of the goals of IaC is to eliminate snowflake systems. A snowflake is a configuration or build that is different from any other. The lack of consistency—or drift—in the platform environment leads to security issues, such as patches that have not been installed, and stability issues, such as scripts that fail to run because of some small configuration difference. By rejecting manual configuration of any kind, IaC ensures idempotence. [[Idempotence means that making the same call with the same parameters will always produce the same result]]. Note that IaC is not simply a matter of using scripts to create instances. Running scripts that have been written ad hoc is just as likely to cause [[environment drift]] as manual configuration. IaC means using carefully developed and tested scripts and orchestration runbooks to generate consistent builds.
# SOFTWARE-DEFINED NETWORKING

IaC is partly facilitated by physical and virtual network appliances that are fully configurable via scripting and APIs. As networks become more complex—perhaps involving thousands of physical and virtual computers and appliances—it becomes more difficult to implement network policies, such as ensuring security and managing traffic flow. With so many devices to configure, it is better to take a step back and consider an[[ abstracted model about how the network functions.]] In this model, network functions can be divided into three "planes":

-   **Control plane**—makes decisions about how traffic should be prioritized and secured, and where it should be switched.
-   **Data plane**—handles the actual switching and routing of traffic and imposition of security access controls.
-   **Management plane**—monitors traffic conditions and network status.

A software-defined networking ([[SDN]]) application can be used to define policy decisions on the control plane. These decisions are then implemented on the data plane by a network controller application, which interfaces with the network devices using APIs. The interface between the SDN applications and the SDN controller is described as the "**northbound**" API, while that between the controller and appliances is the "**southbound**" API. SDN can be used to manage compatible physical appliances, but also virtual switches, routers, and firewalls. The architecture supporting rapid deployment of virtual networking using general-purpose VMs and containers is called network functions virtualization (NFV) ([redhat.com/en/topics/virtualization/what-is-nfv](https://www.redhat.com/en/topics/virtualization/what-is-nfv)).

This architecture saves network and security administrators the job and complexity of configuring each appliance with proper settings to enforce the desired policy. It also allows for fully automated deployment (or provisioning) of network links, appliances, and servers. This makes SDN an important part of the latest automation and orchestration technologies.
# SOFTWARE-DEFINED VISIBILITY

Where [[SDN]] addresses secure network "build" solutions, software-defined visibility (SDV) supports assessment and incident response functions. Visibility is the near real-time collection, aggregation, and reporting of data about network traffic flows and the configuration and status of all the hosts, applications, and user accounts participating in it.

[[SDV]] can help the security data collection process by gathering statistics from the forwarding systems and then applying a classification scheme to those systems to detect network traffic that deviates from baseline levels ([gigamon.com/content/dam/resource-library/english/white-paper/wp-software-defined-visibility-new-paradigm-for-it.pdf](https://wmx-api-production.s3.amazonaws.com/courses/5731/supplementary/wp-software-defined-visibility-new-paradigm-for-it.pdf)). This can provide you with a more robust ability to detect anomalies—anomalies that may suggest an incident. SDV therefore gives you a high-level perspective of network flow and endpoint/user account behavior that may not be possible with traditional appliances. SDV supports designs such as zero trust and east/west ([paloaltonetworks.com/cyberpedia/what-is-a-zero-trust-architecture](https://www.paloaltonetworks.com/cyberpedia/what-is-a-zero-trust-architecture)), plus implementation of security orchestration and automated response (SOAR).
# FOG AND EDGE COMPUTING

Most of the cloud services we have considered so far are "[[user-facing]]." They support applications that human users interact with, such as video streaming, CRM, business analytics, email and conferencing, endpoint protection analytics, and so on. However, a very large and increasing amount of cloud data processing takes place with data generated by Internet of Things (IoT) devices and sensors. Industrial processes and even home automation are availability-focused. While confidentiality and integrity are still important concerns, service interruption in an operational technology network can be physically dangerous. Consequently, there is a strong requirement to retrieve and analyze IoT data with low latency.

A traditional data center architecture does not meet this requirement very well. Sensors are quite likely to have relatively **low-bandwidth, higher latency** WAN links to data networks. Sensors may generate huge quantities of data only a selection of which needs to be prioritized for analysis. Fog computing, developed by Cisco ([cisco.com/c/dam/en_us/solutions/trends/iot/docs/computing-overview.pdf](https://wmx-api-production.s3.amazonaws.com/courses/5731/supplementary/computing-overview.pdf)), addresses these requirements by placing fog node processing resources close to the physical location for the IoT sensors. The sensors communicate with the fog node, using Wi-Fi, ZigBee, or 4G/5G, and the fog node prioritizes traffic, analyzes and remediates alertable conditions, and backhauls remaining data to the data center for storage and low-priority analysis.

Edge computing is a broader concept partially developed from fog computing and partially evolved in parallel to it. Fog computing is now seen as working within the concept of edge computing.[[ Edge computing]] uses the following concepts:

-   **Edge devices** are those that collect and depend upon data for their operation. For example, a thermometer in an HVAC system collects temperature data; the controller in an HVAC system activates the electromechanical components to turn the heating or air conditioning on or off in response to ambient temperature changes. The impact of latency becomes apparent when you consider edge devices such as self-driving automobiles. 
-   **Edge gateways** perform some pre-processing of data to and from edge devices to enable prioritization. They also perform the wired or wireless connectivity to transfer data to and from the storage and processing networks.
-   **Fog nodes** can be incorporated as a data processing layer positioned close to the edge gateways, assisting the prioritization of critical data transmission.
-   The **cloud or data center** layer provides the main storage and processing resources, plus distribution and aggregation of data between sites.

In security terms, the fog node or edge gateway layers represent high-value targets for both denial of service and data exfiltration attacks. 

The controversy over the use of Huawei's equipment within 5G and edge networks illustrates the risks and concerns over supply chains and trusted computing ([threatpost.com/huawei-5g-security-implications/152926](https://threatpost.com/huawei-5g-security-implications/152926/)).

 